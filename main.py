import os
import random
import time
import copy
from collections import Counter

import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
from torchvision import transforms, models
from PIL import Image
import matplotlib.pyplot as plt
import openpyxl  # –Ω—É–∂–µ–Ω –¥–ª—è —á—Ç–µ–Ω–∏—è .xlsx (pandas –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–∞–∫ –¥–≤–∏–∂–æ–∫)

# =========================
# –ü—É—Ç–∏ –∏ –±–∞–∑–æ–≤—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏
# =========================
XLS_PATH = "data/point.xlsx"  # –§–∞–π–ª —Å –º–µ—Ç–∫–∞–º–∏ (.xlsx)
IMG_DIR = "data/Images"  # –ü–∞–ø–∫–∞ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏

if not os.path.exists(XLS_PATH):
    raise FileNotFoundError(f"‚ùå –§–∞–π–ª —Å –º–µ—Ç–∫–∞–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {XLS_PATH}")
if not os.path.exists(IMG_DIR):
    raise FileNotFoundError(f"‚ùå –ü–∞–ø–∫–∞ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {IMG_DIR}")

print(f"‚úÖ –ù–∞–π–¥–µ–Ω —Ñ–∞–π–ª –º–µ—Ç–æ–∫: {XLS_PATH}")
print(f"‚úÖ –ù–∞–π–¥–µ–Ω–∞ –ø–∞–ø–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {IMG_DIR}")


# =========================
# –§–∏–∫—Å–∞—Ü–∏—è —Å–ª—É—á–∞–π–Ω–æ—Å—Ç–∏
# =========================
def set_seed(seed: int = 42):
    random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


set_seed(42)

# =========================
# –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏
# =========================
# –ü–æ–ø—Ä–æ–±—É–µ–º –≤–∑—è—Ç—å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é –∏–∑ –≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–∏; –µ—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∏—Ç—Å—è ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º ImageNet-—Å—Ç–∞–Ω–¥–∞—Ä—Ç
try:
    from torchvision.models import ResNet18_Weights

    weights = ResNet18_Weights.DEFAULT
    imagenet_mean = weights.meta["mean"]
    imagenet_std = weights.meta["std"]
except Exception:
    imagenet_mean = [0.485, 0.456, 0.406]
    imagenet_std = [0.229, 0.224, 0.225]

transform_train = transforms.Compose([
    transforms.Resize((512, 512)),
    # –õ—ë–≥–∫–∞—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è (–º–æ–∂–Ω–æ —É–±—Ä–∞—Ç—å, –µ—Å–ª–∏ —ç—Ç–æ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –Ω—É–∂–Ω–∞ —Å—Ç—Ä–æ–≥–∞—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å)
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.ToTensor(),
    transforms.Normalize(mean=imagenet_mean, std=imagenet_std),
])

transform_val = transforms.Compose([
    transforms.Resize((512, 512)),
    transforms.ToTensor(),
    transforms.Normalize(mean=imagenet_mean, std=imagenet_std),
])


# =========================
# –ö–∞—Å—Ç–æ–º–Ω—ã–π Dataset
# =========================
class ClassificationDataset(Dataset):
    def __init__(self, img_dir, xls_path, transform=None):
        self.img_dir = img_dir
        self.transform = transform
        try:
            df = pd.read_excel(xls_path, engine="openpyxl")
        except Exception as e:
            raise RuntimeError(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å Excel-—Ñ–∞–π–ª: {e}")

        required_cols = ['dicom_name', '–≠–∫—Å–ø–µ—Ä—Ç 1', '–≠–∫—Å–ø–µ—Ä—Ç 2', '–≠–∫—Å–ø–µ—Ä—Ç 3']
        for col in required_cols:
            if col not in df.columns:
                raise ValueError(f"‚ùå –í Excel-—Ñ–∞–π–ª–µ –Ω–µ—Ç —Å—Ç–æ–ª–±—Ü–∞: '{col}'. –ï—Å—Ç—å: {list(df.columns)}")

        # –£–±–∏—Ä–∞–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏
        df = df.dropna(subset=required_cols).copy()

        # –ü—Ä–∏–≤–æ–¥–∏–º –æ—Ü–µ–Ω–∫–∏ –∫ int
        df[['–≠–∫—Å–ø–µ—Ä—Ç 1', '–≠–∫—Å–ø–µ—Ä—Ç 2', '–≠–∫—Å–ø–µ—Ä—Ç 3']] = df[['–≠–∫—Å–ø–µ—Ä—Ç 1', '–≠–∫—Å–ø–µ—Ä—Ç 2', '–≠–∫—Å–ø–µ—Ä—Ç 3']].astype(int)

        self.img_names = []
        self.labels = []

        for _, row in df.iterrows():
            votes = [row['–≠–∫—Å–ø–µ—Ä—Ç 1'], row['–≠–∫—Å–ø–µ—Ä—Ç 2'], row['–≠–∫—Å–ø–µ—Ä—Ç 3']]

            # –ï—Å–ª–∏ —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω —ç–∫—Å–ø–µ—Ä—Ç –ø–æ—Å—Ç–∞–≤–∏–ª -1 ‚Äî –∏—Å–∫–ª—é—á–∞–µ–º
            if -1 in votes:
                continue

            # –ú–∞–∂–æ—Ä–∏—Ç–∞—Ä–Ω–æ–µ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ
            majority_label = Counter(votes).most_common(1)[0][0]

            self.img_names.append(str(row['dicom_name']))
            self.labels.append(int(majority_label))

        if len(self.img_names) == 0:
            raise RuntimeError("‚ùå –ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –Ω–µ –æ—Å—Ç–∞–ª–æ—Å—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –º–µ—Ç–∫–∏ –∏ –∑–Ω–∞—á–µ–Ω–∏—è -1.")

        print(f"‚úÖ –î–∞—Ç–∞—Å–µ—Ç —Å–æ–∑–¥–∞–Ω: –≤–∫–ª—é—á–µ–Ω–æ {len(self.img_names)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (–±–µ–∑ –º–µ—Ç–æ–∫ -1)")

    def __len__(self):
        return len(self.img_names)

    def __getitem__(self, idx):
        img_name = self.img_names[idx]
        label = self.labels[idx]

        # –ò—â–µ–º —Ñ–∞–π–ª –ø–æ –≤–æ–∑–º–æ–∂–Ω—ã–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è–º
        img_path = None
        for ext in ['.png', '.jpg', '.jpeg']:
            candidate = os.path.join(self.img_dir, img_name + ext)
            if os.path.exists(candidate):
                img_path = candidate
                break

        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ ‚Äî –ø—Ä–æ–±—É–µ–º –∏–º—è –∫–∞–∫ –µ—Å—Ç—å (–≤–¥—Ä—É–≥ —É–∂–µ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ–º)
        if img_path is None:
            img_path = os.path.join(self.img_dir, img_name)
            if not os.path.exists(img_path):
                raise FileNotFoundError(f"‚ùå –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ: {img_path}")

        # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        try:
            image = Image.open(img_path)
            if image.mode != 'RGB':
                image = image.convert('RGB')
        except Exception as e:
            raise IOError(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {img_path}: {e}")

        # –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏
        if self.transform:
            image = self.transform(image)

        return image, torch.tensor(label, dtype=torch.long)


# =========================
# –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏ –±—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π
# =========================
full_dataset = ClassificationDataset(IMG_DIR, XLS_PATH, transform=transform_val)  # –¥–ª—è –ø—Ä–µ–≤—å—é –±–µ–∑ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏

# –ë–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤
counts = Counter(full_dataset.labels)
print(f"üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤: {dict(counts)} (0 = '–ù–µ—Ç –≥—Ä—ã–∂–∏', 1 = '–ï—Å—Ç—å –≥—Ä—ã–∂–∞')")

# –¢–µ—Å—Ç: –ø–æ–∫–∞–∑–∞—Ç—å –ø–µ—Ä–≤—ã–π –±–∞—Ç—á –∏–∑ 4 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
print("\nüîç –ü–†–û–í–ï–†–ö–ê –î–ê–¢–ê–°–ï–¢–ê: –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–π –±–∞—Ç—á –∏–∑ 4 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
_preview_loader = DataLoader(full_dataset, batch_size=4, shuffle=True, num_workers=0)
images, labels = next(iter(_preview_loader))
print(f"üìå –§–æ—Ä–º–∞ —Ç–µ–Ω–∑–æ—Ä–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {images.shape}")  # [4, 3, 512, 512]
print(f"üìå –ú–µ—Ç–∫–∏: {labels.tolist()}")


# –î–µ-–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è –ø–æ–∫–∞–∑–∞
def denormalize(tensor, mean, std):
    mean = torch.tensor(mean).view(1, 3, 1, 1)
    std = torch.tensor(std).view(1, 3, 1, 1)
    return tensor * std + mean


fig, axes = plt.subplots(1, 4, figsize=(16, 4))
class_names = {0: "–ù–µ—Ç –≥—Ä—ã–∂–∏", 1: "–ï—Å—Ç—å –≥—Ä—ã–∂–∞"}
imgs_to_show = denormalize(images.clone(), imagenet_mean, imagenet_std)

for i in range(min(4, images.size(0))):
    img = imgs_to_show[i].permute(1, 2, 0).numpy().clip(0, 1)
    axes[i].imshow(img)
    axes[i].set_title(f"{class_names.get(labels[i].item(), labels[i].item())}", fontsize=10)
    axes[i].axis("off")

plt.tight_layout()
plt.show()

# =========================
# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/val –∏ DataLoader'—ã
# =========================
# 80/20 —Å–ø–ª–∏—Ç
train_ratio = 0.8
train_size = int(train_ratio * len(full_dataset))
val_size = len(full_dataset) - train_size

# –í–∞–∂–Ω–æ: –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ä–∞–∑–Ω—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ (train/val)
# –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–∏–º transform —á–µ—Ä–µ–∑ Subset —Å –¥–æ—Å—Ç—É–ø–æ–º –∫ –∏—Å—Ö–æ–¥–Ω–æ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É
# –ü—Ä–æ—â–µ ‚Äî —Å–æ–∑–¥–∞—Ç—å –¥–≤–∞ –Ω–æ–≤—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å —Ç–µ–º–∏ –∂–µ –∏–º–µ–Ω–∞–º–∏/–º–µ—Ç–∫–∞–º–∏, –Ω–æ —Ä–∞–∑–Ω—ã–º–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞–º–∏
indices = list(range(len(full_dataset)))
random.shuffle(indices)
train_indices = indices[:train_size]
val_indices = indices[train_size:]


# –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è "–æ–±—ë—Ä—Ç–∫–∞" –Ω–∞–¥ –±–∞–∑–æ–≤—ã–º –¥–∞—Ç–∞—Å–µ—Ç–æ–º —Å –¥—Ä—É–≥–∏–º transform
class WrappedDataset(Dataset):
    def __init__(self, base_dataset: ClassificationDataset, indices, transform):
        self.base = base_dataset
        self.indices = indices
        self.transform = transform

    def __len__(self):
        return len(self.indices)

    def __getitem__(self, i):
        idx = self.indices[i]
        # –ë–µ—Ä—ë–º –∏—Å—Ö–æ–¥–Ω—ã–π –ø—É—Ç—å –∏ –º–µ—Ç–∫—É –∫–æ—Å–≤–µ–Ω–Ω–æ: –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–≥–∏–∫—É base, –Ω–æ –º–µ–Ω—è–µ–º transform –Ω–∞ –ª–µ—Ç—É
        img_name = self.base.img_names[idx]
        label = self.base.labels[idx]

        # –ü–æ–≤—Ç–æ—Ä —Ç–æ–π –∂–µ –ª–æ–≥–∏–∫–∏ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞, —á—Ç–æ –∏ –≤ base (—á—Ç–æ–±—ã –Ω–µ –¥—É–±–ª–∏—Ä–æ–≤–∞—Ç—å, –º–æ–∂–Ω–æ –±—ã–ª–æ –±—ã –≤—ã–Ω–µ—Å—Ç–∏ –º–µ—Ç–æ–¥)
        img_path = None
        for ext in ['.png', '.jpg', '.jpeg']:
            candidate = os.path.join(self.base.img_dir, img_name + ext)
            if os.path.exists(candidate):
                img_path = candidate
                break
        if img_path is None:
            img_path = os.path.join(self.base.img_dir, img_name)
            if not os.path.exists(img_path):
                raise FileNotFoundError(f"‚ùå –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ: {img_path}")

        image = Image.open(img_path)
        if image.mode != 'RGB':
            image = image.convert('RGB')

        if self.transform:
            image = self.transform(image)

        return image, torch.tensor(label, dtype=torch.long)


train_dataset = WrappedDataset(full_dataset, train_indices, transform_train)
val_dataset = WrappedDataset(full_dataset, val_indices, transform_val)

# DataLoader'—ã
use_cuda = torch.cuda.is_available()
device = torch.device("cuda" if use_cuda else "cpu")
kwargs = dict(num_workers=0, pin_memory=True) if use_cuda else dict(num_workers=0)

train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, **kwargs)
val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, **kwargs)

print(f"\nüß© –†–∞–∑–º–µ—Ä—ã: train={len(train_dataset)}, val={len(val_dataset)}")
print(f"üñ•Ô∏è –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")


# =========================
# –ú–æ–¥–µ–ª—å
# =========================
def build_model(num_classes=2):
    try:
        from torchvision.models import ResNet18_Weights
        weights = ResNet18_Weights.DEFAULT
        model = models.resnet18(weights=weights)
    except Exception:
        model = models.resnet18(pretrained=True)

    # –ó–∞–º–µ–Ω–∞ –≥–æ–ª–æ–≤–æ–π –ø–æ–¥ –Ω–∞—à—É –∑–∞–¥–∞—á—É
    num_features = model.fc.in_features
    model.fc = nn.Linear(num_features, num_classes)
    return model


model = build_model(num_classes=2).to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=1e-4)


# =========================
# –¶–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è
# =========================
def train_model(model, criterion, optimizer, train_loader, val_loader, device, num_epochs=10):
    best_wts = copy.deepcopy(model.state_dict())
    best_acc = 0.0

    for epoch in range(num_epochs):
        print(f"\nEpoch {epoch + 1}/{num_epochs}")
        epoch_start = time.time()

        for phase, loader in [('train', train_loader), ('val', val_loader)]:
            if phase == 'train':
                model.train()
            else:
                model.eval()

            running_loss = 0.0
            running_corrects = 0
            total = 0

            for inputs, labels in loader:
                inputs = inputs.to(device)
                labels = labels.to(device)
                batch_size = inputs.size(0)

                optimizer.zero_grad()
                with torch.set_grad_enabled(phase == 'train'):
                    outputs = model(inputs)
                    loss = criterion(outputs, labels)
                    _, preds = torch.max(outputs, 1)

                    if phase == 'train':
                        loss.backward()
                        optimizer.step()

                running_loss += loss.item() * batch_size
                running_corrects += torch.sum(preds == labels).item()
                total += batch_size

            epoch_loss = running_loss / total
            epoch_acc = running_corrects / total
            print(f"{phase:>5} | Loss: {epoch_loss:.4f} | Acc: {epoch_acc:.4f}")

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –ø–æ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            if phase == 'val' and epoch_acc > best_acc:
                best_acc = epoch_acc
                best_wts = copy.deepcopy(model.state_dict())

        print(f"‚è±Ô∏è –í—Ä–µ–º—è —ç–ø–æ—Ö–∏: {time.time() - epoch_start:.1f} —Å")

    print(f"\nüèÜ –õ—É—á—à–∞—è val Acc: {best_acc:.4f}")
    model.load_state_dict(best_wts)
    return model


model = train_model(model, criterion, optimizer, train_loader, val_loader, device, num_epochs=10)

# =========================
# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
# =========================
os.makedirs("outputs", exist_ok=True)
save_path = os.path.join("outputs", "cxr_binary_resnet18.pth")
torch.save(model.state_dict(), save_path)
print(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {save_path}")
